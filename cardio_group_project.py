# -*- coding: utf-8 -*-
"""Cardio_group_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IxJ_QjI8Qq7-BT5TSlbDPyOSSNR8hJUM
"""

import os
# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version
# For example:
# spark_version = ‘spark-3.0.1’
spark_version = "spark-3.0.1"
os.environ["SPARK_VERSION"]=spark_version
# Install Spark and Java
!apt-get update
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz
!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz
!pip install -q findspark
# Set Environment Variables
os.environ['JAVA_HOME'] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ['SPARK_HOME'] = f"/content/{spark_version}-bin-hadoop2.7"
# Start a SparkSession
import findspark
findspark.init()
!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar
# Start Spark session
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("ETL").config("spark.driver.extraClassPath", "/content/postgresql-42.2.9.jar").getOrCreate()
# spark = SparkSession.builder.appName("ETL”).config(“spark.driver.extraClassPath”,“/content/postgresql-42.2.9.jar”).getOrCreate()

# checking the dataframe(s) we're uploading 

# url = "https://raw.githubusercontent.com/LingZ1993/blended_project_1/Nick/cardio_complete.csv"

# df = 

df = spark.read.csv("/cardio_complete.csv", header =True)
# df = spark.read.csv(“YOURCSVNAME.csv”, header=True)
df.show()

# Write DataFrame to RDS
mode= "overwrite"
jdbc_url = "jdbc:postgresql://group-cardio-project-database-1993.clviqkfcmhow.us-east-2.rds.amazonaws.com:5432/Cardio_secrets"
# jdbc_url = "jdbc:postgresql://group-cardio-project-database-1993.clviqkfcmhow.us-east-2.rds.amazonaws.com:5432/cardio_secrets”
config = {"user": "postgres",
          "password" : "01eellv01",
          "driver" : "org.postgresql.Driver"}
# config = {"user”:"postgres",
#           "password”: "01eellv01",
#           "driver”:"org.postgresql.Driver"}
df.repartition(100).write.jdbc(url=jdbc_url, table="cardio_complete", mode=mode, properties=config)
# df.repartition(100).write.jdbc(url=jdbc_url, table="cardio_complete", mode=mode, properties=config)


# white_check_mark
# eyes
# raised_hands